# 数据特征选定
对数据进行特征选择，有助于：
+ **降低数据的拟合度：** 较少的冗余数据，得出正确结论机会更大
+ **提高算法精度** 较少的误导数据，能够提高算法准确度
+ **减少训练时间** 越少的数据，训练模型所需时间更少

## 1.单变量特征选定
卡方检验是检验定性自变量对定性因变量的相关性的方法。假设自变量有N个，因变量有M个，考虑自变量等于i且因变量等于j的样本频数的观察值与期望值的差距，构建统计量。卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，卡方值越大，越不符合；卡方值越小，偏差越小，若完全符合，则为0。

```
test = SelectKBest(score_func=chi2, k=4) #选择4个特征
fit = test.fit(X, Y)
features = fit.transform(X)
```

## 2.递归特征消除
递归特征消除（RFE）是一个多轮训练的算法，每轮训练消除若干权值系数特征，然后再基于新的特征集进行下一轮训练。

```
model = LogisticRegression()
rfe = RFE(model, 3)
fit = rfe.fit(X, Y)
print('特征个数:', fit.n_features_)
print('被选定的特征:', fit.support_)
print('特征排名：', fit.ranking_)
```

## 3.主要成分分析
主要成分分析（PCA）是使用线性代数来转换压缩数据，通常被称作数据降维，类似的降维算法还有线性判别分析（LDA）。PCA是无监督降维，让样本具有更大的发散性，LDA是有监督降维方法，让样本有最好的分类性能。

```
pca = PCA(n_components=3) 
fit =pca.fit(X)
print("解释方差：%s" % fit.explained_variance_ratio_)
print(fit.components)
```
## 4.特征重要性
袋装决策树算法、随机森林算法和极端随机树森林算法都可以用来计算数据特征的重要性。

```
model = ExtraTreesClassifier()
fit = model.fit(X, Y)
print(fit.feature_importances_)
```

