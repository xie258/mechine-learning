# 集成算法
* 装袋算法
* 提升算法
* 投票算法

## 一、装袋算法
### 1.装袋决策树
当数据具有很大的方差时非常有效

```
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
cart = DecisionTreeClassifier()
num_tree = 100
model = BaggingClassifier(base_estimator=cart, n_estimators=num_tree, random_state=seed)
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 2.随机森林
随机森林就如同有很多个精通不同领域的专家，对于一个新的问题，可以从不同的角度去看待它，最终由各个专家投票得到结果。

```
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
num_tree = 100
max_features = 3
model = RandomForestClassifier(n_estimators=num_tree, random_state=seed, max_features=max_features)
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 3.极端随机树
随机森林应用的是Bagging模型，二极端随机树是使用所有的训练样本，决策树所用的都是相同的全部训练样本。随机森林是在一个随机子集内得到最优分叉特征属性，而极端随机树是完全随机地选择分叉特征属性，从而实现对决策树进行分叉的。

```
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
num_tree = 100
max_features = 7
model = ExtraTreesClassifier(n_estimators=num_tree, random_state=seed, max_features=max_features)
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

## 二、提升算法
* AdaBoost
* 随机梯度提升

### 1.AdaBoost
AdaBoost是一种迭代算法，针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。算法本身通过改变数据分布来实现，它根据每次训练集中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。它将修改过权值的新数据集送给下层分类器进行训练，再将每次训练得到的分类器融合起来，作为最后的决策分类器。**使用AdaBoost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。**

```
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
num_tree = 30
model = AdaBoostClassifier(n_estimators=num_tree, random_state=seed, max_features=max_features)
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 2.随机梯度提升
随机梯度提升法（GBM）只用一个样本点来更新回归系数，是改良的梯度下降算法。

```
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
num_tree = 100
max_features = 7
model = GradientBoostingClassifier(n_estimators=num_tree, random_state=seed, max_features=max_features)
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

## 三、投票算法
投票算法是通过创建两个或多个算法模型，计算各个子模型的平均预测状况，可以对每个子模型的预测结果增加权重来提高算法的准确度。

```
num_folds = 10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
cart = DecisionTreeClassifier()
models = []
model_logistic = LogisticRegression()
models.append(('logistic', model_logistic))
model_cart = DecisionTreeClassifier()
models.append(('cart', model_cart))
model_svc = SVC()
models.append(('svm), model_svc)
ensemble_model = VotingClassfier(n_estimators=models)
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```