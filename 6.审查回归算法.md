# 审查回归算法

## 一、线性算法
### 1.线性回归算法
y = wx + e ,e 表示误差服从均值为0的正态分布

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = LinearRegression()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Linear Regression: %.3f' % result.mean())
```

### 2.岭回归算法
岭回归算法是一种专门用于共线性数据分析的有偏估计回归算法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = Ridge()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Ridge Regression: %.3f' % result.mean())
```

### 3.套索回归算法
与岭回归算法类似，会惩罚回归系数，能够减少变化程度并提高线性回归模型的精度。岭回归算法惩罚函数是平方，套索是绝对值。如果预测的一组变量高度相似，套索回归会选择其中的一个变量，并将其他的变量收缩为0.
```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = Lasso()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Lasso Regression: %.3f' % result.mean())
```

### 4.弹性网络回归算法
弹性网络回归算法是套索回归和岭回归算法的混合体，当有多个相关的特征时，这个算法很有用。在高度相关变量的情况下，它会产生群体效应；选择变量的的数目没有限制；可以承受双重收缩。

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = ElastcNet()
scoring = 'neg_mean_squred_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('ElasticNet Regression: %.3f' % result.mean())
```

## 二、非线性算法
### 1.K近邻算法
```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = KNeighborsRegressor()
scoring = 'neg_mean_squred_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('KNeighbors Regression: %.3f' % result.mean())
```

### 2.分类与回归树
```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = DecisionRegressor()
scoring = 'neg_mean_squred_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('CART Regression: %.3f' % result.mean())
```

### 3.支持向量机
```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = SVR()
scoring = 'neg_mean_squred_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('SVM: %.3f' % result.mean())
```