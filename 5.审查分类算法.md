# 审查分类算法
审查分类算法是用来判断哪个算法对数据集最有效、能够生成最有模型，审查算法的建议：
* 尝试多种代表性算法
* 尝试多种机器学习的算法
* 尝试多种模型

## 一、线性算法
### 1.逻辑回归
逻辑回归是一个分类算法，根据自变量来判断因变量的真假，适用于二分类

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = LogisticRegression()
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 2.线性判别回归
线性判别回归，将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距和最小的类内间距，即模式在该空间中有最佳的可分离性。

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = LinearDiscriminantAnalysis()
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

## 二、非线性算法
### 1.K近邻算法
将样本分为K类，判断与哪一类的距离最近就属于哪一类

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = KNeighborsClassfifer()
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 2.贝叶斯分类器
对于给出的待分类项，计算出此条件下各个类别出现的概率，哪个最大就认为该类属于这个类别。

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = GaussianNB()
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 3.分类与回归树
分类与回归树（CART），分为二叉树，左分支是“1”，右分支是“0”，根据输入条件输出条件概率分布。

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = DecisionTreeClassfier()
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```

### 4.支持向量机
可以用来分类和回归

```
num_folds =  10
seed = 7
kfold = KFold(n_splits=num_folds, random_state=seed)
model = SVC()
result = cross_val_score(model, X, Y, cv=kfold)
print(result.mean())
```